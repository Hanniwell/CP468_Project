I have two algorithm questions? | Yahoo Answers Home Mail News Sports Finance Celebrity Style Movies Weather Mobile Ask Sign in Mail All Categories Arts & Humanities Beauty & Style Business & Finance Cars & Transportation Computers & Internet Consumer Electronics Dining Out Education & Reference Entertainment & Music Environment Family & Relationships Food & Drink Games & Recreation Health Home & Garden Local Businesses News & Events Pets Politics & Government Pregnancy & Parenting Science & Mathematics Social Science Society & Culture Sports Travel Yahoo Products Anonymous Anonymous
asked in
Science & Mathematics Mathematics
·
10 years ago I have two algorithm questions? 1- Worst-case analysis for cost of an algorithm is often easier to perform than “average” case analysis of cost for the same algorithm. Why might the worst-case analysis be misleading? Why might the “average” case cost be misleading? 2-Big O notation means that the fastest growing term in a function form is proportional to the term inside the O() parentheses. This means that ultimately for sufficiently large N, this term will dominate and be all that matters. Why does this NOT mean that for the size of problems of interest to you, an algorithm whose cost is O(g(N)) will be cheaper than another algorithm whose cost is O(h(N)), where asymptotically g(N)
< h(N)? would appreciate if someone could help me answer this two and probably give me some examples as well Answer Save 1 Answer Relevance Samwise Lv
7 10 years ago Favorite Answer 1. The worst-case analysis might be misleading if the algorithm is to be used for a large number of cases, and the worst case is unlikely to turn up often. Even if it has worse behavior in the worst case, an algorithm with better average efficiency might be more effective in the long run. For example, I had a long argument with a colleague over his plan to implement a connection lookup based on
(a) a hash function using the remote address, which indexed a pointer to (b) a link-list of connections to remote addresses which produced that hash value. His argument was that In the worst case, many connections could be linked together on a lengthy list, but in general the hash function would spread the connections evenly over a set of link-lists, so the average lookup would be much shorter. The average case might be misleading if the worst-case behavior is considerably worse, and if the chances of hitting a worst case are high. For example, in the connection-lookup case mentioned above, I pointed out that there were actually two levels of remote addresses, and that we had customers with equipment that established a large number of connections using the same value for the address on which my colleague wanted to base the hash function. This meant that all connections from that device would be on the same link-list, slowing down their lookup and increasing the processing load of our product. Another example, familiar to many, is the fact that a QuickSort algorithm's average performance is very high, but it deteriorates from O(N log N) to O(N^2) [basically, the same as a bubble sort] if the list happens to be already sorted, or sorted in reverse. The chances of someone re-sorting an already-sorted list are actually pretty high. 2. There are two reasons the theoretically better algorithm might not be best for a particular problem. First, the relationship is only guaranteed "for sufficiently large N." For smaller values of N, the degree to which the term inside the O() parentheses dominates is not guaranteed. If you have small problems, other considerations come into play. Some of those other considerations are on the purely practical side (as the programmer sees it), which is also the theoretical side (as the software engineer sees it). The cost of writing, testing, and maintaining the code is a factor. An algorithm that performs better "for sufficiently large N" may not be worthwhile for small applications, when the cost of implementation and maintenance is figured in, because for small N the difference in cost of using the algorithm might be negligible. Back to my previous example: The solution to the connection-lookup problem was to use the hash function, but instead of link-lists, continue connection lookup by means of balanced B-trees. Had the expected difference in performance, in real-life cases, been minor (as my colleague had imagined), the link-lists would have offered significant advantages in simplicity: less code and much easier maintenance. The choice of the suitable algorithm had to be based on knowledge of the specific environment in which it was to be used. Another example: if you know you'll only be sorting small lists, a bubble sort (O(N^2)) may well be preferable to any of the O(N log N) sorts simply on grounds of easy and obvious coding with less danger of a subtle error somewhere. 0 0 0 Login to reply the answers Post Still have questions? Get your answers by asking now. Ask Question + 100 Join Yahoo Answers and get 100 points today. Join Trending Questions Trending Questions What's the name of the decade that includes 2000 and 1999? 5 answers For how many real value of a will x^2+ 3ax+ 2009
=0 has two integer roots? 7 answers What is the value of pi? ? 26 answers Are inches smaller than feet? 11 answers 2/9 of 36? 8 answers Answer Questions Answer Questions Find the x-coordinate of vertex and y-coordinate of vertex of the following quadratic function.   f(x)=−4x2+4x+1? [Discrete Math] How do you get the CNF of a compound proposition using a truth table? discrete math? Math Project Help? How many lines through the origin make angles of 60^o with both the +y and +z axes ? What angle do they make with the +x axis? Terms ・ Privacy ・ AdChoices ・ RSS ・ Help About Answers ・ Community Guidelines ・ Leaderboard ・ Knowledge Partners ・ Points & Levels Send Feedback ・ International Sites